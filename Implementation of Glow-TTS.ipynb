{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe60cd1c",
   "metadata": {},
   "source": [
    "# Implementation of Glow-TTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ee061",
   "metadata": {},
   "source": [
    "- Github 코드를 이해한 뒤 직접 코드를 구현해본다.\n",
    "- Github 코드를 그대로 따라쓰지 않는다. 충분히 이해한 뒤, 나만의 방식으로 작성하는 것을 목표로 한다.\n",
    "    - 그러나 가장 효율적인 코드는 그대로 따라 쓴 부분이 꽤 있다...\n",
    "- 참고할 Github 코드는 논문 저자의 코드이다: `https://github.com/jaywalnut310/glow-tts`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91f599",
   "metadata": {},
   "source": [
    "# 0. Import Libraries & Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f2108",
   "metadata": {},
   "source": [
    "## 0.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd940c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data\n",
    "import torch\n",
    "import numpy as np\n",
    "import os, glob, librosa, re, scipy\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# 2. Model\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import math\n",
    "import time\n",
    "\n",
    "# 3. Training\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70811b1f",
   "metadata": {},
   "source": [
    "## 0.2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1eaae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major Hyperparameters\n",
    "batch_size = 32\n",
    "logging_step = 10\n",
    "validation_step = 100\n",
    "checkpoint_step = 1000\n",
    "\n",
    "# 1. Data\n",
    "mel_dim = 80\n",
    "\n",
    "# 2.1. Encoder\n",
    "symbol_length = 70 # len(symbols) = 70 (PAD + EOS + VALID_CHARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f8f19",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbb740",
   "metadata": {},
   "source": [
    "- Data는 HiFi-GAN에서 전처리한 데이터를 그대로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e28a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '_'\n",
    "EOS = '~'\n",
    "SPACE = ' '\n",
    "\n",
    "JAMO_LEADS = \"\".join([chr(_) for _ in range(0x1100, 0x1113)])\n",
    "JAMO_VOWELS = \"\".join([chr(_) for _ in range(0x1161, 0x1176)])\n",
    "JAMO_TAILS = \"\".join([chr(_) for _ in range(0x11A8, 0x11C3)])\n",
    "\n",
    "VALID_CHARS = JAMO_LEADS + JAMO_VOWELS + JAMO_TAILS + SPACE\n",
    "symbols = PAD + EOS + VALID_CHARS\n",
    "\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
    "\n",
    "# text를 초성, 중성, 종성으로 분리하여 id로 반환하는 함수\n",
    "def text_to_sequence(text):\n",
    "    sequence = []\n",
    "    if not 0x1100 <= ord(text[0]) <= 0x1113:\n",
    "        text = ''.join(list(hangul_to_jamo(text)))\n",
    "    for s in text:\n",
    "        sequence.append(_symbol_to_id[s])\n",
    "    sequence.append(_symbol_to_id['~'])\n",
    "    return sequence\n",
    "\n",
    "def sequence_to_text(sequence):\n",
    "    result = ''\n",
    "    for symbol_id in sequence:\n",
    "        if symbol_id in _id_to_symbol:\n",
    "            s = _id_to_symbol[symbol_id]\n",
    "            result += s\n",
    "    return result.replace('}{', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270fbd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/Poco/Jupyter/PaperReview/dataset/kss/data\"\n",
    "\n",
    "class TextMelDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.text_list = sorted(glob.glob(os.path.join(data_dir + '/text', '*.npy')))\n",
    "        self.mel_list = sorted(glob.glob(os.path.join(data_dir + '/mel', '*.npy')))\n",
    "        self.wav_list = sorted(glob.glob(os.path.join(data_dir + '/wav', '*.npy')))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.from_numpy(np.load(self.text_list[idx]))\n",
    "        text_len = len(text)\n",
    "        \n",
    "        mel = torch.from_numpy(np.load(self.mel_list[idx]))\n",
    "        mel_len = mel.shape[0]\n",
    "        \n",
    "        wav = torch.from_numpy(np.load(self.wav_list[idx]))\n",
    "        wav_len = wav.shape[0]\n",
    "        return (text, text_len, mel, mel_len, wav, wav_len)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text = []\n",
    "    text_len = []\n",
    "    mel = []\n",
    "    mel_len = []\n",
    "    wav = []\n",
    "    wav_len = []\n",
    "    \n",
    "    for t, tl, m, ml, w, wl in batch:\n",
    "        text.append(t)\n",
    "        text_len.append(tl)\n",
    "        mel.append(m)\n",
    "        mel_len.append(ml)\n",
    "        wav.append(w)\n",
    "        wav_len.append(wl)\n",
    "        \n",
    "    max_text_len = max(text_len)\n",
    "    max_mel_len = max(mel_len)\n",
    "    max_wav_len = max(wav_len)\n",
    "    \n",
    "    # text zero_padding\n",
    "    padded_text_batch = torch.zeros((len(batch), max_text_len), dtype=torch.int32)\n",
    "    for i, x in enumerate(text):\n",
    "        padded_text_batch[i, :len(x)] = torch.Tensor(x)\n",
    "    \n",
    "    # mel zero_padding\n",
    "    padded_mel_batch = torch.zeros((len(batch), max_mel_len, mel_dim), dtype=torch.float32)\n",
    "    for i, x in enumerate(mel):\n",
    "        padded_mel_batch[i, :x.shape[0], :x.shape[1]] = torch.Tensor(x)\n",
    "        \n",
    "    # wav zero_padding\n",
    "    padded_wav_batch = torch.zeros((len(batch), max_wav_len), dtype=torch.float32)\n",
    "    for i, x in enumerate(wav):\n",
    "        padded_wav_batch[i, :x.shape[0]] = torch.Tensor(x)\n",
    "        \n",
    "    return padded_text_batch, text_len, padded_mel_batch, mel_len, padded_wav_batch, wav_len\n",
    "\n",
    "dataset = TextMelDataset(data_dir)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "train_ratio = 0.99\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True,\n",
    "                              batch_size=batch_size, collate_fn=collate_fn,\n",
    "                              pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset,  shuffle=False,\n",
    "                            batch_size=1, collate_fn=collate_fn,\n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b72a042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[0](text) shape(B, Max_T): torch.Size([32, 82])\n",
      "batch[1](text_len) len(B): 32\n",
      "batch[2](mel) shape(B, Max_F, mel_dim): torch.Size([32, 532, 80])\n",
      "batch[3](mel_len) len(B): 32\n",
      "batch[4](wav) shape(B, Max_L): torch.Size([32, 136192])\n",
      "batch[5](wav_len) len(B): 32\n",
      "Total: 12854\n",
      "num_of_batches: 398\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 객체를 반복자로 변환\n",
    "dataiter = iter(train_dataloader)\n",
    "\n",
    "# 데이터 한 번 추출\n",
    "batch = next(dataiter)\n",
    "\n",
    "print('batch[0](text) shape(B, Max_T):', batch[0].shape) # Max_T: The number of text length\n",
    "print('batch[1](text_len) len(B):', len(batch[1]))\n",
    "print('batch[2](mel) shape(B, Max_F, mel_dim):', batch[2].shape) # Max_F: The number of Mel-spectrogram frames\n",
    "print('batch[3](mel_len) len(B):', len(batch[3]))\n",
    "print('batch[4](wav) shape(B, Max_L):', batch[4].shape) # Max_L: length of time(seconds) * sampling_rate(22050)\n",
    "print('batch[5](wav_len) len(B):', len(batch[5]))\n",
    "print('Total:', dataset.__len__())\n",
    "print('num_of_batches:', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21afeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[0](text) shape(B, Max_T): torch.Size([1, 65])\n",
      "batch[1](text_len) len(B): 1\n",
      "batch[2](mel) shape(B, Max_F, mel_dim): torch.Size([1, 412, 80])\n",
      "batch[3](mel_len) len(B): 1\n",
      "batch[4](wav) shape(B, Max_L): torch.Size([1, 105472])\n",
      "batch[5](wav_len) len(B): 1\n",
      "Total: 12854\n",
      "num_of_batches: 129\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 객체를 반복자로 변환\n",
    "dataiter = iter(val_dataloader)\n",
    "\n",
    "# 데이터 한 번 추출\n",
    "batch = next(dataiter)\n",
    "\n",
    "print('batch[0](text) shape(B, Max_T):', batch[0].shape) # Max_T: The number of text length\n",
    "print('batch[1](text_len) len(B):', len(batch[1]))\n",
    "print('batch[2](mel) shape(B, Max_F, mel_dim):', batch[2].shape) # Max_F: The number of Mel-spectrogram frames\n",
    "print('batch[3](mel_len) len(B):', len(batch[3]))\n",
    "print('batch[4](wav) shape(B, Max_L):', batch[4].shape) # Max_L: length of time(seconds) * sampling_rate(22050)\n",
    "print('batch[5](wav_len) len(B):', len(batch[5]))\n",
    "print('Total:', dataset.__len__())\n",
    "print('num_of_batches:', len(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94651d85",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6940a5f",
   "metadata": {},
   "source": [
    "## 2.1. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690cedc",
   "metadata": {},
   "source": [
    "### 2.1.1. Encoder Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fb6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    여러 곳에서 정규화(Norm)를 위해 사용되는 모듈.\n",
    "    \n",
    "    nn.LayerNorm이 이미 pytorch 안에 구현되어 있으나, 항상 마지막 차원을 정규화한다.\n",
    "    그래서 channel을 기준으로 정규화하는 LayerNorm을 따로 구현한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        channels: 입력 데이터의 channel 수 | LayerNorm은 channel 차원을 정규화한다.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.eps = 1e-4\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.ones(channels)) # 학습 가능한 파라미터\n",
    "        self.beta = nn.Parameter(torch.zeros(channels)) # 학습 가능한 파라미터\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, channels, *) | 정규화할 입력 데이터\n",
    "        =====outputs=====\n",
    "        x: (B, channels, *) | channel 차원이 정규화된 데이터\n",
    "        \"\"\"\n",
    "        mean = torch.mean(x, dim=1, keepdim=True) # channel 차원(index=1)의 평균 계산, 차원을 유지한다.\n",
    "        variance = torch.mean((x-mean)**2, dim=1, keepdim=True) # 분산 계산\n",
    "        \n",
    "        x = (x - mean) * (variance + self.eps)**(-0.5) # (x - m) / sqrt(v)\n",
    "        \n",
    "        n = len(x.shape)\n",
    "        shape = [1] * n\n",
    "        shape[1] = -1 # shape = [1, -1, 1] or [1, -1, 1, 1]\n",
    "        x = x * self.gamma.view(*shape) + self.beta.view(*shape) # y = x*gamma + beta\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PreNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder의 1번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        for i in range(3):\n",
    "            self.convs.append(nn.Conv1d(192, 192, kernel_size=5, padding=2)) # (B, 192, T) 유지\n",
    "            self.norms.append(LayerNorm(192)) # (B, 192, T) 유지\n",
    "        self.linear = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지 | linear 역할을 하는 conv\n",
    "        \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, T) | Embedding된 입력 데이터\n",
    "        x_mask: (B, 1, T) | 글자 길이에 따른 mask (글자가 있으면 True, 없으면 False로 구성)\n",
    "        =====outputs=====\n",
    "        x: (B, 192, T)\n",
    "        \"\"\"\n",
    "        x0 = x\n",
    "        for i in range(3):\n",
    "            x = self.convs[i](x * x_mask)\n",
    "            x = self.norms[i](x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = x0 + x # residual connection\n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder 중 2번째 모듈인 TransformerEncoder의 1번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_heads = 2\n",
    "        self.window_size = 4\n",
    "        self.k_channels = 192 // self.n_heads # 96\n",
    "        \n",
    "        self.linear_q = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지\n",
    "        self.linear_k = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지\n",
    "        self.linear_v = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지\n",
    "        nn.init.xavier_uniform_(self.linear_q.weight)\n",
    "        nn.init.xavier_uniform_(self.linear_k.weight)\n",
    "        nn.init.xavier_uniform_(self.linear_v.weight)\n",
    "        \n",
    "        relative_std = self.k_channels ** (-0.5) # 0.1xx\n",
    "        self.relative_k = nn.Parameter(torch.randn(1, self.window_size * 2 + 1, self.k_channels) * relative_std) # (1, 9, 96)\n",
    "        self.relative_v = nn.Parameter(torch.randn(1, self.window_size * 2 + 1, self.k_channels) * relative_std) # (1, 9, 96)\n",
    "        \n",
    "        self.attention_weights = None\n",
    "        self.linear_out = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, query, context, attention_mask, self_attention=True):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        query: (B, 192, T_target) | Glow-TTS에서는 self-attention만 이용하므로 query와 context가 동일한 텐서 x이다.\n",
    "        context: (B, 192, T_source) | query = context || 여기에서는 특히 T_source = T_target 이다.\n",
    "        attention_mask: (B, 1, T, T) | x_mask.unsqueeze(2) * z_mask.unsqueeze(3)\n",
    "        self_attention: True/False | self_attention일 때 relative position representations를 적용한다. 여기에서는 항상 True이다.\n",
    "        # 실제로는 query와 context에 같은 텐서 x를 입력하면 된다.\n",
    "        =====outputs=====\n",
    "        output: (B, 192, T)\n",
    "        \"\"\"\n",
    "        \n",
    "        query = self.linear_q(query)\n",
    "        key = self.linear_k(context)\n",
    "        value = self.linear_v(context)\n",
    "        \n",
    "        B, _, T_tar = query.size()\n",
    "        T_src = key.size(2)\n",
    "        query = query.view(B, self.n_heads, self.k_channels, T_tar).transpose(2, 3)\n",
    "        key = key.view(B, self.n_heads, self.k_channels, T_src).transpose(2, 3)\n",
    "        value = value.view(B, self.n_heads, self.k_channels, T_src).transpose(2, 3)\n",
    "            # (B, 192, T_src) -> (B, 2, 96, T_src) -> (B, 2, T_src, 96)\n",
    "            \n",
    "        scores = torch.matmul(query, key.transpose(2, 3)) / (self.k_channels ** 0.5)\n",
    "            # (B, 2, T_tar, 96) * (B, 2, 96, T_src) -> (B, 2, T_tar, T_src)\n",
    "        \n",
    "        if self_attention: # True\n",
    "            # Get relative embeddings (relative_keys) (1-1)\n",
    "            padding = max(T_src - (self.window_size + 1), 0) # max(T-5, 0)\n",
    "            start_pos = max((self.window_size + 1) - T_src, 0) # max(5-T, 0)\n",
    "            end_pos = start_pos + 2 * T_src - 1 # (2*T-1) or (T+4)\n",
    "            relative_keys = F.pad(self.relative_k, (0, 0, padding, padding))\n",
    "                # (1, 9, 96) -> (1, pad+9+pad, 96) = (1, 2T-1, 96)\n",
    "            \"\"\"\n",
    "            위 코드의 F.pad(input, pad) 에서 pad = (0, 0, padding, padding)은 다음을 의미한다.\n",
    "            - 앞의 (0, 0): input의 -1차원을 앞으로 0, 뒤로 0만큼 패딩한다.\n",
    "            - 앞의 (padding, padding): input의 -2차원을 앞으로 padding, 뒤로 padding만큼 패딩한다.\n",
    "            즉, F.pad에서 pad는 역순으로 생각해주어야 한다.\n",
    "            \"\"\"\n",
    "            relative_keys = relative_keys[:, start_pos:end_pos, :] # (1, 2T-1, 96)\n",
    "            \n",
    "            # Matmul with relative keys (2-1)\n",
    "            relative_keys = relative_keys.unsqueeze(0).transpose(2, 3) # (1, 2T-1, 96) -> (1, 1, 2T-1, 96) -> (1, 1, 96, 2T-1)\n",
    "            x = torch.matmul(query, relative_keys) # (B, 2, T_tar, 96) * (1, 1, 96, 2T_src-1) = (B, 2, T, 2T-1)\n",
    "                # self attention에서는 T_tar = T_src이므로 이를 다르게 고려할 필요가 없다.\n",
    "            \n",
    "            # Relative position to absolute position (3-1)\n",
    "            T = T_tar # Absolute position to relative position에서도 쓰임.\n",
    "            x = F.pad(x, (0, 1)) # (B, 2, T, 2*T-1) -> (B, 2, T, 2*T)\n",
    "            x = x.view(B, self.n_heads, T * 2 * T) # (B, 2, T, 2*T) -> (B, 2. 2T^2)\n",
    "            x = F.pad(x, (0, T-1)) # (B, 2, 2T^2 + T - 1)\n",
    "            x = x.view(B, self.n_heads, T+1, 2*T-1) # (B, 2, T+1, 2T-1)\n",
    "            relative_logits = x[:, :, :T, T-1:] # (B, 2, T, T)\n",
    "            \n",
    "            # Compute scores\n",
    "            scores_local = relative_logits / (self.k_channels ** 0.5)\n",
    "            scores = scores + scores_local # (B, 2, T, T)\n",
    "            \"\"\"\n",
    "            위 식은 Self-Attention with Relative Position Representations 논문의 5번 식을 구현한 것이다.\n",
    "            Relative- 논문: https://arxiv.org/pdf/1803.02155.pdf\n",
    "            \"\"\"\n",
    "\n",
    "        scores = scores.masked_fill(attention_mask == 0, -1e-4) # attention_mask가 0인 곳을 -1e-4로 채운다.\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1) # (B, 2, T_tar, T_src) # Relative- 논문에서의 alpha에 해당한다.\n",
    "        attention_weights = self.dropout(attention_weights) # dropout하는 이유가 무엇일까?\n",
    "        output = torch.matmul(attention_weights, value) # (B, 2, T_tar, T_src) * (B, 2, T_src, 96) -> (B, 2, T_tar, 96)\n",
    "        \n",
    "        if self_attention: # True\n",
    "            # Absolute position to relative position (3-2)\n",
    "            x = F.pad(attention_weights, (0, T-1)) # (B, 2, T, T) -> (B, 2, T, 2T-1)\n",
    "            x = x.view((B, self.n_heads, T * (2*T-1))) # (B, 2, 2T^2-T)\n",
    "            x = F.pad(x, (T, 0)) # (B, 2, 2T^2) # 앞에 패딩\n",
    "            x = x.view((B, self.n_heads, T, 2*T)) # (B, 2, T, 2T)\n",
    "            relative_weights = x[:, :, :, 1:] # (B, 2, T, 2T-1)\n",
    "            \n",
    "            # Get relative embeddings (relative_value) (1-2) # (1-1)과 거의 동일\n",
    "            padding = max(T_src - (self.window_size + 1), 0) # max(T-5, 0)\n",
    "            start_pos = max((self.window_size + 1) - T_src, 0) # max(5-T, 0)\n",
    "            end_pos = start_pos + 2 * T_src - 1 # (2*T-1) or (T+4)\n",
    "            relative_values = F.pad(self.relative_v, (0, 0, padding, padding))\n",
    "                # (1, 9, 96) -> (1, pad+9+pad, 96) = (1, 2T-1, 96)\n",
    "            relative_values = relative_values[:, start_pos:end_pos, :] # (1, 2T-1, 96)\n",
    "\n",
    "            # Matmul with relative values (2-2)\n",
    "            relative_values = relative_values.unsqueeze(0) # (1, 1, 2T-1, 96)\n",
    "\n",
    "            output = output + torch.matmul(relative_weights, relative_values)\n",
    "                # (B, 2, T, 2T-1) * (1, 1, 2T-1, 96) = (B, 2, T, 96)\n",
    "            \"\"\"\n",
    "            위 식은 Self-Attention with Relative Position Representations 논문의 3번 식을 구현한 것이다. (분배법칙 이용)\n",
    "            Relative- 논문: https://arxiv.org/pdf/1803.02155.pdf\n",
    "            \"\"\"\n",
    "        \n",
    "        output = output.transpose(2, 3).contiguous().view(B, 192, T_tar)\n",
    "            # (B, 2, 96, T) -> 메모리에 연속 배치 -> (B, 192, T)\n",
    "            \n",
    "        self.attention_weights = attention_weights # (B, 2, T, T)\n",
    "        output = self.linear_out(output)\n",
    "        return output # (B, 192, T)\n",
    "    \n",
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder 중 2번째 모듈인 TransformerEncoder의 2번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(192, 768, kernel_size=3, padding=1) # (B, 192, T) -> (B, 768, T)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(768, 192, kernel_size=3, padding=1) # (B, 768, T) -> (B, 192, T)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, T)\n",
    "        x_mask: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        output: (B, 192, T)\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        output = x * x_mask\n",
    "        return output\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder의 2번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attentions = nn.ModuleList()\n",
    "        self.norms1 = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        self.norms2 = nn.ModuleList()\n",
    "        for i in range(6):\n",
    "            self.attentions.append(MultiHeadAttention())\n",
    "            self.norms1.append(LayerNorm(192))\n",
    "            self.ffns.append(FFN())\n",
    "            self.norms2.append(LayerNorm(192))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, T)\n",
    "        x_mask: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        output: (B, 192, T)\n",
    "        \"\"\"\n",
    "        attention_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(3)\n",
    "            # (B, 1, 1, T) * (B, 1, T, 1) = (B, 1, T, T), only consist 0 or 1\n",
    "        for i in range(6):\n",
    "            x = x * x_mask\n",
    "            y = self.attentions[i](x, x, attention_mask)\n",
    "            y = self.dropout(y)\n",
    "            x = x + y # residual connection\n",
    "            x = self.norms1[i](x) # (B, 192, T) 유지\n",
    "            \n",
    "            y = self.ffns[i](x, x_mask)\n",
    "            y = self.dropout(y)\n",
    "            x = x + y # residual connection\n",
    "            x = self.norms2[i](x)\n",
    "        output = x * x_mask\n",
    "        return output # (B, 192, T)\n",
    "    \n",
    "class DurationPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder의 3번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(192, 256, kernel_size=3, padding=1) # (B, 192, T) -> (B, 256, T)\n",
    "        self.norm1 = LayerNorm(256)\n",
    "        self.conv2 = nn.Conv1d(256, 256, kernel_size=3, padding=1) # (B, 256, T) -> (B, 256, T)\n",
    "        self.norm2 = LayerNorm(256)\n",
    "        self.linear = nn.Conv1d(256, 1, kernel_size=1) # (B, 256, T) -> (B, 1, T)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, T)\n",
    "        x_mask: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        output: (B, 1, T)\n",
    "        \"\"\"\n",
    "        x = self.conv1(x * x_mask) # (B, 192, T) -> (B, 256, T)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv2(x * x_mask) # (B, 256, T) -> (B, 256, T)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear(x * x_mask) # (B, 256, T) -> (B, 1, T)\n",
    "        output = x * x_mask\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5bcbfa",
   "metadata": {},
   "source": [
    "### 2.1.2. Main Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b687f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(symbol_length, 192) # (B, T) -> (B, T, 192)\n",
    "        nn.init.normal_(self.embedding.weight, 0.0, 192**(-0.5)) # 가중치 정규분포 초기화 (N(0, 0.07xx))\n",
    "        \n",
    "        self.prenet = PreNet()\n",
    "        self.transformer_encoder = TransformerEncoder()\n",
    "        self.project_mean = nn.Conv1d(192, 80, kernel_size=1) # (B, 192, T) -> (B, 80, T)\n",
    "        self.project_std = nn.Conv1d(192, 80, kernel_size=1) # (B, 192, T) -> (B, 80, T)\n",
    "        \n",
    "        self.duration_predictor = DurationPredictor()\n",
    "        \n",
    "    def forward(self, text, text_len):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        text: (B, Max_T)\n",
    "        text_len: (B)\n",
    "        =====outputs=====\n",
    "        x_mean: (B, 80, T) | 평균, 논문 저자 구현의 train.py에서 out_channels를 80으로 설정한 것을 알 수 있음.\n",
    "        x_std: (B, 80, T) | 표준편차\n",
    "        x_dur: (B, 1, T)\n",
    "        x_mask: (B, 1, T)\n",
    "        \"\"\"\n",
    "        x = self.embedding(text) # (B, T) -> (B, T, 192)\n",
    "        x = x.transpose(1, 2) # (B, T, 192) -> (B, 192, T)\n",
    "        \n",
    "        # Make the x_mask\n",
    "        x_mask = torch.zeros_like(x[:, 0:1, :], dtype=torch.bool) # (B, 1, T)\n",
    "        for idx, length in enumerate(text_len):\n",
    "            x_mask[idx, :, :length] = True\n",
    "        \n",
    "        x = self.prenet(x, x_mask) # (B, 192, T)\n",
    "        x = self.transformer_encoder(x, x_mask) # (B, 192, T)\n",
    "        \n",
    "        # project\n",
    "        x_mean = self.project_mean(x) # (B, 192, T) -> (B, 80, T)\n",
    "        x_std = self.project_std(x) # (B, 192, T) -> (B, 80, T)\n",
    "        \n",
    "        # duration predictor\n",
    "        x_dp = torch.detach(x) # stop_gradient\n",
    "        x_dur = self.duration_predictor(x, x_mask) # (B, 192, T) -> (B, 1, T)\n",
    "        \n",
    "        return x_mean, x_std, x_dur, x_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a15a880",
   "metadata": {},
   "source": [
    "## 2.2. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d97cbc",
   "metadata": {},
   "source": [
    "### 2.2.1. Decoder Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfdd2b",
   "metadata": {},
   "source": [
    "- 필요성을 확인할 수 없기에 다음을 무시하고 구현한다. 나중에 필요할 때 추가로 구현하라.\n",
    "    - g: global condition (n_speaker)\n",
    "    - ddi (데이터 종속적 초기화(data-dependent initialization))\n",
    "    - no_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c03168ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decoder는 Glow: Generative Flow with Invertible 1×1 Convolutions 논문의 기본 구조를 따라간다.\n",
    "Glow 논문: https://arxiv.org/pdf/1807.03039.pdf\n",
    "\"\"\"\n",
    "def Squeeze(x, x_mask):\n",
    "    \"\"\"\n",
    "    Decoder의 preprocessing\n",
    "    =====inputs=====\n",
    "    x: (B, 80, F) | mel_spectrogram or latent representation\n",
    "    x_mask: (B, 1, F)\n",
    "    =====outputs=====\n",
    "    x: (B, 160, F//2) | F//2 = [F/2] ([]: 가우스 기호)\n",
    "    x_mask: (B, 160, F//2)\n",
    "    \"\"\"\n",
    "    B, C, F = x.size()\n",
    "    x = x[:, :, :(F//2)*2] # F가 홀수이면 맨 뒤 한 frame을 버림.\n",
    "    x = x.view(B, C, F//2, 2) # (B, 80, F//2, 2)\n",
    "    x = x.permute(0, 3, 1, 2).contiguous() # (B, 2, 80, F//2)\n",
    "    x = x.view(B, C*2, F//2) # (B, 160, F//2)\n",
    "    \n",
    "    x_mask = x_mask[:, :, 1::2] # (B, 1, F//2) frame을 1부터 한칸씩 건너뛴다.\n",
    "    x = x * x_mask # masking\n",
    "    return x, x_mask\n",
    "\n",
    "class ActNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder의 1번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_s = nn.Parameter(torch.zeros(1, 160, 1)) # Glow 논문의 s에서 log를 취한 것이다. 즉, log[s]\n",
    "        self.bias = nn.Parameter(torch.zeros(1, 160, 1))\n",
    "        \n",
    "    def forward(self, x, x_mask, reverse=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 160, F//2) | mel_spectrogram features\n",
    "        x_mask: (B, 1, F//2) | mel_spectrogram features의 mask. (Decoder의 Squeeze에서 변형됨.)\n",
    "        =====outputs=====\n",
    "        z: (B, 160, F//2)\n",
    "        log_det: (B) or None | log_determinant, reverse=True이면 None 반환\n",
    "        \"\"\"\n",
    "        x_len = torch.sum(x_mask, [1, 2]) # (B) | 1, 2차원의 값을 더한다. cf. [1, 2] 대신 [2]만 사용하면 shape가 (B, 1)이 된다.\n",
    "        \n",
    "        if not reverse:\n",
    "            z = (x * torch.exp(self.log_s) + self.bias) * x_mask # function & masking\n",
    "            log_det = x_len * torch.sum(self.log_s) # log_determinant\n",
    "                # Glow 논문의 Table 1을 확인하라. log_s를 log[s]라 볼 수 있다.\n",
    "                # determinant 대신 log_determinant를 사용하는 이유는 det보다 작은 수치와 적은 계산량 때문으로 추측된다.\n",
    "        else:\n",
    "            z = ((x - self.bias) / torch.exp(self.log_s)) * x_mask # inverse function & masking\n",
    "            log_det = None\n",
    "        \n",
    "        return z, log_det\n",
    "\n",
    "class InvertibleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder의 2번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        Q = torch.linalg.qr(torch.FloatTensor(4, 4).normal_())[0] # (4, 4)\n",
    "        \"\"\"\n",
    "        torch.FloatTensor(4, 4).normal_(): 정규분포 N(0, 1)에서 무작위로 추출한 4x4 matrix\n",
    "        Q, R = torch.linalg.qr(W): QR분해 | Q: 직교 행렬, R: upper traiangular 행렬 cf. det(Q) = 1 or -1\n",
    "        \"\"\"\n",
    "        if torch.det(Q) < 0:\n",
    "            Q[:, 0] = -1 * Q[:, 0] # 0번째 열의 부호를 바꿔서 det(Q) = -1로 만든다.\n",
    "        self.W = nn.Parameter(Q)\n",
    "    \n",
    "    def forward(self, x, x_mask, reverse=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 160, F//2)\n",
    "        x_mask: (B, 1, F//2)\n",
    "        =====outputs=====\n",
    "        z: (B, 160, F//2)\n",
    "        log_det: (B) or None\n",
    "        \"\"\"\n",
    "        B, C, f = x.size() # B, 160, F//2\n",
    "        x_len = torch.sum(x_mask, [1, 2]) # (B)\n",
    "        \n",
    "        # channel mixing\n",
    "        x = x.view(B, 2, C//4, 2, f) # (B, 2, 40, 2, F//2)\n",
    "        x = x.permute(0, 1, 3, 2, 4).contiguous() # (B, 2, 2, 40, F//2)\n",
    "        x = x.view(B, 4, C//4, f) # (B, 4, 40, F//2)\n",
    "        \n",
    "        # 편의상 log_det부터 구한다.\n",
    "        if not reverse:\n",
    "            weight = self.W\n",
    "            log_det = (C/4) * x_len * torch.logdet(self.W) # (B) | torch.logdet(W): log(det(W))\n",
    "                # height = C/4, width = x_len 인 상황임을 고려하면 Glow 논문의 log_determinant 식과 같다.\n",
    "        else:\n",
    "            weight = torch.linalg.inv(self.W) # inverse matrix\n",
    "            log_det = None\n",
    "        \n",
    "        weight = weight.view(4, 4, 1, 1)\n",
    "        z = F.conv2d(x, weight) # (B, 4, 40, F//2) * (4, 4, 1, 1) -> (B, 4, 40, F//2)\n",
    "        \"\"\"\n",
    "        F.conv2d(x, weight)의 convolution 연산은 다음과 같이 생각해야 한다.\n",
    "        (B, 4, 40, F//2): (batch_size, in_channels, height, width)\n",
    "        (4, 4, 1, 1): (out_channels, in_channels/groups, kernel_height, kernel_width)\n",
    "        \n",
    "        즉, nn.Conv2d(4, 4, kernel_size=(1, 1))인 상황에 가중치를 준 것이다.\n",
    "        \"\"\"\n",
    "        \n",
    "        # channel unmixing\n",
    "        z = z.view(B, 2, 2, C//4, f) # (B, 4, 40, F//2) -> (B, 2, 2, 40, F//2)\n",
    "        z = z.permute(0, 1, 3, 2, 4).contiguous() # (B, 2, 40, 2, F//2)\n",
    "        z = z.view(B, C, f) * x_mask # (B, 160, F//2) & masking\n",
    "        return z, log_det\n",
    "    \n",
    "class WN(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder의 3번째 모듈인 AffineCouplingLayer의 모듈\n",
    "    \n",
    "    해당 구조는 WAVEGLOW: A FLOW-BASED GENERATIVE NETWORK FOR SPEECH SYNTHESIS 로부터 제안되었다.\n",
    "    WaveGlow 논문: https://arxiv.org/pdf/1811.00002.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, dilation_rate=1):\n",
    "        super().__init__()\n",
    "        self.in_layers = nn.ModuleList()\n",
    "        self.res_skip_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(4):\n",
    "            dilation = dilation_rate ** i # NVIDIA WaveGlow에서는 dilation_rate=2이지만, 여기에서는 1이므로 의미는 없다.\n",
    "            in_layer = weight_norm(nn.Conv1d(192, 2*192, kernel_size=5, dilation=dilation,\n",
    "                                 padding=((5-1) * dilation)//2)) # (B, 192, F//2) -> (B, 2*192, F//2)\n",
    "            self.in_layers.append(in_layer)\n",
    "            \n",
    "            if i < 3:\n",
    "                res_skip_layer = weight_norm(nn.Conv1d(192, 2*192, kernel_size=1)) # (B, 192, F//2) -> (B, 2*192, F//2)\n",
    "            else:\n",
    "                res_skip_layer = weight_norm(nn.Conv1d(192, 192, kernel_size=1)) # (B, 192, F//2) -> (B, 192, F//2)\n",
    "            self.res_skip_layers.append(res_skip_layer)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.05)\n",
    "    \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, F//2)\n",
    "        x_mask: (B, 1, F//2)\n",
    "        =====outputs=====\n",
    "        output: (B, 192, F//2)\n",
    "        \"\"\"\n",
    "        output = torch.zeros_like(x) # (B, 192, F//2) all zeros\n",
    "        \n",
    "        for i in range(4):\n",
    "            x_in = self.in_layers[i](x) # (B, 192, F//2) -> (B, 2*192, F//2)\n",
    "            x_in = self.dropout(x_in) # dropout\n",
    "            \n",
    "            # fused add tanh sigmoid multiply\n",
    "            tanh_act = torch.tanh(x_in[:, :192, :]) # (B, 192, F//2)\n",
    "            sigmoid_act = torch.sigmoid(x_in[:, 192:, :]) # (B, 192, F//2)\n",
    "            \n",
    "            acts = sigmoid_act * tanh_act # (B, 192, F//2)\n",
    "            \n",
    "            x_out = self.res_skip_layers[i](acts) # (B, 192, F//2) -> (B, 2*192, F//2) or [last](B, 192, F//2)\n",
    "            if i < 3:\n",
    "                x = (x + x_out[:, :192, :]) * x_mask # residual connection & masking\n",
    "                output += x_out[:, 192:, :] # add output\n",
    "            else:\n",
    "                output += x_out # (B, 192, F//2)\n",
    "        \n",
    "        output = output * x_mask # masking\n",
    "        return output\n",
    "\n",
    "class AffineCouplingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder의 3번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.start_conv = weight_norm(nn.Conv1d(160//2, 192, kernel_size=1)) # (B, 80, F//2) -> (B, 192, F//2)\n",
    "        self.wn = WN()\n",
    "        self.end_conv = nn.Conv1d(192, 160, kernel_size=1) # (B, 192, F//2) -> (B, 160, F//2)\n",
    "        # end_conv의 초기 가중치를 0으로 설정하는 것이 처음에 학습하지 않는 역할을 하며, 이는 학습 안정화에 도움이 된다.\n",
    "        self.end_conv.weight.data.zero_() # weight를 0으로 초기화\n",
    "        self.end_conv.bias.data.zero_() # bias를 0으로 초기화\n",
    "        \n",
    "    def forward(self, x, x_mask, reverse=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 160, F//2)\n",
    "        x_mask: (B, 1, F//2)\n",
    "        =====outputs=====\n",
    "        z: (B, 160, F//2)\n",
    "        log_det: (B) or None\n",
    "        \"\"\"\n",
    "        B, C, f = x.size() # B, 160, F//2\n",
    "        x_0, x_1 = x[:, :C//2, :], x[:, C//2:, :] # split: (B, 80, F//2) x2\n",
    "        \n",
    "        x = self.start_conv(x_0) * x_mask # (B, 80, F//2) -> (B, 192, F//2) & masking\n",
    "        x = self.wn(x, x_mask) # (B, 192, F//2)\n",
    "        out = self.end_conv(x) # (B, 192, F//2) -> (B, 160, F//2)\n",
    "        \n",
    "        z_0 = x_0 # (B, 80, F//2)\n",
    "        m = out[:, :C//2, :] # (B, 80, F//2)\n",
    "        log_s = out[:, C//2:, :] # (B, 80, F//2)\n",
    "        \n",
    "        if not reverse:\n",
    "            z_1 = (torch.exp(log_s) * x_1 + m) * x_mask # (B, 80, F//2) | function & masking \n",
    "            log_det = torch.sum(log_s * x_mask, [1, 2]) # (B)\n",
    "        else:\n",
    "            z_1 = (x_1 - m) / torch.exp(log_s) * x_mask # (B, 80, F//2) | inverse function & masking\n",
    "            log_det = None\n",
    "        \n",
    "        z = torch.cat([z_0, z_1], dim=1) # (B, 160, F//2)\n",
    "        return z, log_det\n",
    "    \n",
    "def Unsqueeze(x, x_mask):\n",
    "    \"\"\"\n",
    "    Decoder의 postprocessing\n",
    "    =====inputs=====\n",
    "    x: (B, 160, F//2)\n",
    "    x_mask: (B, 1, F//2)\n",
    "    =====outputs=====\n",
    "    x: (B, 80, F)\n",
    "    x_mask: (B, 1, F)\n",
    "    \"\"\"\n",
    "    B, C, f = x.size() # B, 160, F//2\n",
    "    x = x.view(B, 2, C//2, f) # (B, 2, 80, F//2)\n",
    "    x = x.permute(0, 2, 3, 1).contiguous() # (B, 80, F//2, 2)\n",
    "    x = x.view(B, C//2, 2*f) # (B, 160, F)\n",
    "    \n",
    "    x_mask = x_mask.unsqueeze(3).repeat(1, 1, 1, 2).view(B, 1, 2*f) # (B, 1, F//2, 1) -> (B, 1, F//2, 2) -> (B, 1, F)\n",
    "    x = x * x_mask # masking\n",
    "    return x, x_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba03d9",
   "metadata": {},
   "source": [
    "### 2.2.2. Main Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f4689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList()\n",
    "        for i in range(12):\n",
    "            self.flows.append(ActNorm())\n",
    "            self.flows.append(InvertibleConv())\n",
    "            self.flows.append(AffineCouplingLayer())\n",
    "        \n",
    "    def forward(self, x, x_mask, reverse=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 80, F) | mel-spectrogram(Direct) OR latent representation(Inverse)\n",
    "        x_mask: (B, 1, F)\n",
    "        =====outputs=====\n",
    "        z: (B, 80, F) | latent representation(Direct) OR mel-spectrogram(Inverse)\n",
    "        total_log_det: (B) or None | log determinant\n",
    "        \"\"\"\n",
    "        if not reverse:\n",
    "            flows = self.flows\n",
    "            total_log_det = 0\n",
    "        else:\n",
    "            flows = reversed(self.flows)\n",
    "            total_log_det = None\n",
    "        \n",
    "        x, x_mask = Squeeze(x, x_mask) # (B, 80, F) -> (B, 160, F//2) | (B, 1, F) -> (B, 1, F//2)\n",
    "        \n",
    "        for f in flows:\n",
    "            if not reverse:\n",
    "                x, log_det = f(x, x_mask, reverse=reverse)\n",
    "                total_log_det += log_det\n",
    "            else:\n",
    "                x, _ = f(x, x_mask, reverse=reverse)\n",
    "                \n",
    "        x, x_mask = Unsqueeze(x, x_mask) # (B, 160, F//2) -> (B, 80, F) | (B, 1, F//2) -> (B, 1, F)\n",
    "        \n",
    "        return x, total_log_det"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdeb2f0",
   "metadata": {},
   "source": [
    "## 2.3. Generator (Encoder+Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ee5bd",
   "metadata": {},
   "source": [
    "### 2.3.1. Generator Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b25065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAS(path, logp, T_max, F_max):\n",
    "    \"\"\"\n",
    "    Glow-TTS의 모듈인 maximum_path의 모듈\n",
    "    MAS 알고리즘을 수행하는 함수이다.\n",
    "    =====inputs=====\n",
    "    path: (T, F)\n",
    "    logp: (T, F)\n",
    "    T_max: (1)\n",
    "    F_max: (1)\n",
    "    =====outputs=====\n",
    "    path: (T, F) | 0과 1로 구성된 alignment\n",
    "    \"\"\"\n",
    "    neg_inf = -1e9 # negative infinity\n",
    "    # forward\n",
    "    for j in range(F_max):\n",
    "        for i in range(max(0, T_max + j - F_max), min(T_max, j + 1)): # 평행사변형을 생각하라.\n",
    "            # Q_i_j-1 (current)\n",
    "            if i == j:\n",
    "                Q_cur = neg_inf\n",
    "            else:\n",
    "                Q_cur = logp[i, j-1] # j=0이면 i도 0이므로 j-1을 사용해도 된다.\n",
    "            \n",
    "            # Q_i-1_j-1 (previous)\n",
    "            if i==0:\n",
    "                if j==0:\n",
    "                    Q_prev = 0. # i=0, j=0인 경우에는 logp 값만 반영해야 한다.\n",
    "                else:\n",
    "                    Q_prev = neg_inf # i=0인 경우에는 Q_i-1_j-1을 반영하지 않아야 한다.\n",
    "            else:\n",
    "                Q_prev = logp[i-1, j-1]\n",
    "            \n",
    "            # logp에 Q를 갱신한다.\n",
    "            logp[i, j] = max(Q_cur, Q_prev) + logp[i, j]\n",
    "    \n",
    "    # backtracking\n",
    "    idx = T_max - 1\n",
    "    for j in range(F_max-1, -1, -1): # F_max-1부터 -1까지(-1 포함 없이 0까지) -1씩 감소\n",
    "        path[idx, j] = 1\n",
    "        if idx != 0:\n",
    "            if (logp[idx, j-1] < logp[idx-1, j-1]) or (idx == j):\n",
    "                idx -= 1\n",
    "                \n",
    "    return path\n",
    "        \n",
    "\n",
    "def maximum_path(logp, attention_mask):\n",
    "    \"\"\"\n",
    "    Glow-TTS에 사용되는 모듈\n",
    "    MAS를 사용하여 alignment를 찾아주는 역할을 한다.\n",
    "    논문 저자 구현에서는 cpython을 이용하여 병렬 처리를 구현한 듯 하나\n",
    "    여기에서는 python만을 이용하여 구현하였다.\n",
    "    =====inputs=====\n",
    "    logp: (B, T, F) | N(x_mean, x_std)의 log-likelihood\n",
    "    attention_mask: (B, T, F)\n",
    "    =====outputs=====\n",
    "    path: (B, T, F) | alignment\n",
    "    \"\"\"\n",
    "    B = logp.shape[0]\n",
    "    \n",
    "    logp = logp*attention_mask\n",
    "    # 계산은 CPU에서 실행되도록 하기 위해 기존의 device를 저장하고 .cpu().numpy()를 한다.\n",
    "    logp_device = logp.device\n",
    "    logp_type = logp.dtype\n",
    "    logp = logp.data.cpu().numpy().astype(np.float32)\n",
    "    attention_mask = attention_mask.data.cpu().numpy()\n",
    "    \n",
    "    path = np.zeros_like(logp).astype(np.int32) # (B, T, F)\n",
    "    T_max = attention_mask.sum(1)[:, 0].astype(np.int32) # (B)\n",
    "    F_max = attention_mask.sum(2)[:, 0].astype(np.int32) # (B)\n",
    "    \n",
    "    # MAS 알고리즘\n",
    "    for idx in range(B):\n",
    "        path[idx] = MAS(path[idx], logp[idx], T_max[idx], F_max[idx]) # (T, F)\n",
    "    return torch.from_numpy(path).to(device=logp_device, dtype=logp_type)\n",
    "\n",
    "def generate_path(ceil_dur, attention_mask):\n",
    "    \"\"\"\n",
    "    Glow-TTS에 사용되는 모듈\n",
    "    inference 과정에서 alignment를 만들어낸다.\n",
    "    =====input=====\n",
    "    ceil_dur: (B, T) | 추론한 duration에 ceil 연산한 것 | ex) [[2, 1, 2, 2, ...], [1, 2, 1, 3, ...], ...]\n",
    "    attention_mask: (B, T, F)\n",
    "    =====output=====\n",
    "    path: (B, T, F) | alignment\n",
    "    \"\"\"\n",
    "    B, T, Frame = attention_mask.shape\n",
    "    cum_dur = torch.cumsum(ceil_dur, 1)\n",
    "    cum_dur = cum_dur.to(torch.int32) # (B, T) | 누적합 | ex) [[2, 3, 5, 7, ...], [1, 3, 4, 7, ...], ...]\n",
    "    path = torch.zeros(B, T, Frame).to(ceil_dur.device) # (B, T, F) | all False(0)\n",
    "    \n",
    "    # make the sequence_mask\n",
    "    for b, batch_cum_dur in enumerate(cum_dur):\n",
    "        for t, each_cum_dur in enumerate(batch_cum_dur):\n",
    "            path[b, t, :each_cum_dur] = torch.ones((1, 1, each_cum_dur)).to(ceil_dur.device)\n",
    "                # cum_dur로부터 True(1)를 path에 새겨넣는다.\n",
    "    path = path - F.pad(path, (0, 0, 1, 0, 0, 0))[:, :-1] # (B, T, F)\n",
    "    \"\"\"\n",
    "    ex) batch를 잠시 제외해두고 예시를 든다.\n",
    "    [[1, 1, 0, 0, 0, 0, 0],   [[0, 0, 0, 0, 0, 0, 0],    [[1, 1, 0, 0, 0, 0, 0],\n",
    "     [1, 1, 1, 0, 0, 0, 0], -  [1, 1, 0, 0, 0, 0, 0],  =  [0, 0, 1, 0, 0, 0, 0],\n",
    "     [1, 1, 1, 1, 1, 0, 0],    [1, 1, 1, 0, 0, 0, 0],     [0, 0, 0, 1, 1, 0, 0],\n",
    "     [1, 1, 1, 1, 1, 1, 1]]    [1, 1, 1, 1, 1, 0, 0]]     [0, 0, 0, 0, 0, 1, 1]]\n",
    "    \"\"\"\n",
    "    path = path * attention_mask\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dfaa46",
   "metadata": {},
   "source": [
    "### 2.3.2. Main Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6664fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlowTTS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, text, text_len, mel=None, mel_len=None, inference=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        text: (B, T)\n",
    "        text_len: (B) list\n",
    "        mel: (B, 80, F)\n",
    "        mel_len: (B) list\n",
    "        inference: True/False\n",
    "        =====outputs=====\n",
    "        (tuple) (z, z_mean, z_log_std, log_det, z_mask)\n",
    "            z(training) or y(inference): (B, 80, F) | z: latent representation, y: mel-spectrogram\n",
    "            z_mean: (B, 80, F)\n",
    "            z_log_std: (B, 80, F)\n",
    "            log_det: (B) or None\n",
    "            z_mask: (B, 1, F)\n",
    "        (tuple) (x_mean, x_log_std, x_mask)\n",
    "            x_mean: (B, 80, T)\n",
    "            x_log_std: (B, 80, T)\n",
    "            x_mask: (B, 1, T)\n",
    "        (tuple) (attention_alignment, x_log_dur, log_d)\n",
    "            attention_alignment: (B, T, F)\n",
    "            x_log_dur: (B, 1, T) | 추측한 duration의 log scale\n",
    "            log_d: (B, 1, T) | 적절하다고 추측한 alignment에서의 duration의 log scale\n",
    "        \"\"\"\n",
    "        x_mean, x_log_std, x_log_dur, x_mask = self.encoder(text, text_len)\n",
    "            # x_std, x_dur 에 log를 붙인 이유는, 논문 저자의 구현에서는 log가 취해진 값으로 간주하기 때문이다.\n",
    "        y, y_len = mel, mel_len\n",
    "        \n",
    "        if not inference: # training\n",
    "            y_max_len = y.size(2)\n",
    "        else: # inference\n",
    "            dur = torch.exp(x_log_dur) * x_mask # (B, 1, T)\n",
    "            ceil_dur = torch.ceil(dur) # (B, 1, T)\n",
    "            y_len = torch.clamp_min(torch.sum(ceil_dur, [1, 2]), 1).long() # (B)\n",
    "                # ceil_dur을 [1, 2] 축에 대해 sum한 뒤 최솟값이 1이상이 되도록 설정. 정수 long 타입으로 반환한다.\n",
    "            y_max_len = None\n",
    "        \n",
    "        # preprocessing\n",
    "        if y_max_len is not None:\n",
    "            y_max_len = (y_max_len // 2) * 2 # 홀수면 1을 빼서 짝수로 만든다.\n",
    "            y = y[:, :, :y_max_len] # y_max_len에 맞게 y를 조정\n",
    "            y_len = (y_len // 2) * 2 # y_len이 홀수이면 1을 빼서 짝수로 만든다.\n",
    "        \n",
    "        # make the z_mask\n",
    "        B = len(y_len)\n",
    "        temp_max = max(y_len)\n",
    "        z_mask = torch.zeros((B, 1, temp_max), dtype=torch.bool).to(device) # (B, 1, F)\n",
    "        for idx, length in enumerate(y_len):\n",
    "            z_mask[idx, :, :length] = True\n",
    "        \n",
    "        # make the attention_mask\n",
    "        attention_mask = x_mask.unsqueeze(3) * z_mask.unsqueeze(2) # (B, 1, T, 1) * (B, 1, 1, F) = (B, 1, T, F)\n",
    "            # 주의: Encoder의 attention_mask와는 다른 mask임.\n",
    "        \n",
    "        if not inference: # training\n",
    "            z, log_det = self.decoder(y, z_mask, reverse=False)\n",
    "            with torch.no_grad():\n",
    "                x_std_squared_root = torch.exp(-2 * x_log_std) # (B, 80, T)\n",
    "                logp1 = torch.sum(-0.5 * math.log(2 * math.pi) - x_log_std, [1]).unsqueeze(-1) # [(B, T, F)\n",
    "                logp2 = torch.matmul(x_std_squared_root.transpose(1, 2), -0.5 * (z ** 2)) # [(B, T, 80) * (B, 80, F) = (B, T, F)\n",
    "                logp3 = torch.matmul((x_mean * x_std_squared_root).transpose(1,2), z) # (B, T, 80) * (B, 80, F) = (B, T, F)\n",
    "                logp4 = torch.sum(-0.5 * (x_mean ** 2) * x_std_squared_root, [1]).unsqueeze(-1) # (B, T, F)\n",
    "                logp = logp1 + logp2 + logp3 + logp4 # (B, T, F)\n",
    "                \"\"\"\n",
    "                logp는 normal distribution N(x_mean, x_std)의 maximum log-likelihood이다.\n",
    "                sum(log(N(z;x_mean, x_std)))를 정규분포 식을 이용하여 분배법칙으로 풀어내면 위와 같은 식이 도출된다.\n",
    "                \"\"\"\n",
    "                attention_alignment = maximum_path(logp, attention_mask.squeeze(1)).detach() # alignment (B, T, F)\n",
    "             \n",
    "            z_mean = torch.matmul(attention_alignment.transpose(1, 2), x_mean.transpose(1, 2)) # (B, F, T) * (B, T, 80) -> (B, F, 80)\n",
    "            z_mean = z_mean.transpose(1, 2) # (B, 80, F)\n",
    "            z_log_std = torch.matmul(attention_alignment.transpose(1, 2), x_log_std.transpose(1, 2)) # (B, F, T) * (B, T, 80) -> (B, F, 80)\n",
    "            z_log_std = z_log_std.transpose(1, 2) # (B, 80, F)\n",
    "            log_d = torch.log(1e-8 + torch.sum(attention_alignment, -1)).unsqueeze(1) * x_mask # (B, 1, T) | alignment에서 형성된 duration의 log scale\n",
    "            return (z, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d)\n",
    "            \n",
    "        else: # inference\n",
    "            # generate_path (make attention_alignment using ceil(x_dur))\n",
    "            attention_alignment = generate_path(ceil_dur.squeeze(1), attention_mask.squeeze(1)) # (B, T, F)\n",
    "            z_mean = torch.matmul(attention_alignment.transpose(1, 2), x_mean.transpose(1, 2)) # (B, F, T) * (B, T, 80) -> (B, F, 80)\n",
    "            z_mean = z_mean.transpose(1, 2) # (B, 80, F)\n",
    "            z_log_std = torch.matmul(attention_alignment.transpose(1, 2), x_log_std.transpose(1, 2)) # (B, F, T) * (B, T, 80) -> (B, F, 80)\n",
    "            z_log_std = z_log_std.transpose(1, 2) # (B, 80, F)\n",
    "            log_d = torch.log(1e-8 + torch.sum(attention_alignment, -1)).unsqueeze(1) * x_mask # (B, 1, T) | alignment에서 형성된 duration의 log scale\n",
    "            \n",
    "            z = (z_mean + torch.exp(z_log_std) * torch.randn_like(z_mean)) * z_mask # z(latent representation) 생성\n",
    "            y, log_det = self.decoder(z, z_mask, reverse=True) # mel-spectrogram 생성\n",
    "            return (y, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f77f629",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a89d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 80, 420])\n",
      "cuda: 3.7896714210510254s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# data\n",
    "batch = next(iter(train_dataloader))\n",
    "text = batch[0].to(device)\n",
    "text_len = torch.tensor(batch[1], dtype=torch.int32)\n",
    "mel = batch[2].to(device).transpose(1, 2)\n",
    "mel_len = torch.tensor(batch[3], dtype=torch.int32)\n",
    "\n",
    "# model\n",
    "model = GlowTTS().to(device)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "(z, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d) = model(\n",
    "    text, text_len, mel, mel_len)\n",
    "print(z.size())\n",
    "\n",
    "end = time.time()\n",
    "print(f\"{device}: {end-start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a64f0",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bad54c",
   "metadata": {},
   "source": [
    "## 3.0. Plot mel-spectrogram & Plot Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d51a38",
   "metadata": {},
   "source": [
    "- 각각 HiFi-GAN과 Tacotron2에서 구현을 가져옴. Plot Alignment는 모델에 맞게 변형함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c8ae053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mel(mel, step=None, loss=None):\n",
    "    \"\"\"\n",
    "    =====inputs=====\n",
    "    mel: (80, F) # 입력 시 .detach().cpu().numpy() 를 적용하는 것을 잊지 말 것!\n",
    "    \"\"\"\n",
    "    plt.rcParams['axes.unicode_minus'] = False # 마이너스 부호 오류에 대한 해결\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    im = ax.imshow(mel, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.title(f\"Step: {step} | Loss: {loss}\")\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.ylabel(\"frequency_bin\")\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    plt.close()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c485e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('Agg')\n",
    "\n",
    "font_name = fm.FontProperties(fname=\"C:/Users/Poco/Jupyter/PaperReview/dataset/malgun.ttf\").get_name()\n",
    "matplotlib.rc('font', family=font_name, size=14)\n",
    "\n",
    "def plot_alignment(alignment, text, step, loss):\n",
    "    text = text.rstrip('_').rstrip('~')\n",
    "    alignment = alignment[:len(text)]\n",
    "    \n",
    "    # 하나의 그림(fig) 객체와 하나의 축(ax) 객체를 생성\n",
    "    fig, ax = plt.subplots(figsize=(len(text)/3, 5))\n",
    "    # 생성한 축(ax) 객체에 이미지를 출력\n",
    "    im = ax.imshow(np.transpose(alignment), aspect='auto', origin='lower')\n",
    "    \n",
    "    plt.title(f\"step: {step}, loss: {loss:.5f}\", loc=\"center\", pad=10)\n",
    "    plt.xlabel('Encoder timestep')\n",
    "    plt.ylabel('Decoder timestep')\n",
    "    # 공백 문자 ' '를 빈 문자열 ''로 변환\n",
    "    text = [x if x != ' ' else '' for x in list(text)]\n",
    "    # x축의 눈금과 레이블을 설정\n",
    "    plt.xticks(range(len(text)), text)\n",
    "    \n",
    "    # 그래프의 레이아웃을 조정\n",
    "    plt.tight_layout()\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    fig.canvas.draw()\n",
    "    plt.close()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5304a28",
   "metadata": {},
   "source": [
    "## 3.1. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3de069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLE_Loss(z, z_mean, z_log_std, log_det, z_mask): # Maximum Likelihood Estimation\n",
    "    \"\"\"\n",
    "    =====input=====\n",
    "    (1st return tuple of the model)\n",
    "    =====output=====\n",
    "    Loss: (1) | Negative Loglikelihood\n",
    "    \"\"\"\n",
    "    Loss = torch.sum(z_log_std) + 0.5 * torch.sum(torch.exp(-2 * z_log_std) * ((z-z_mean)**2)) # (1)\n",
    "    Loss -= torch.sum(log_det) # (2)\n",
    "    Loss /= torch.sum(torch.ones_like(z) * z_mask) # (3)\n",
    "    Loss += 0.5 * math.log(2 * math.pi) # (4)\n",
    "    \"\"\"\n",
    "    Negative Loglikelihood\n",
    "    (1) + (3)*(4) + (2) : 논문에서 제시된 Loglikelihood에 음을 취한 값 (Negative Loglikelihood)\n",
    "    여기에서 전체에 (3)으로 나눠주었다고 생각하면 된다. (Averaging across B, C=80, F)\n",
    "    \"\"\"\n",
    "    return Loss\n",
    "\n",
    "def Duration_Loss(x_log_dur, log_d, text_len):\n",
    "    \"\"\"\n",
    "    =====inputs=====\n",
    "    x_log_dur: (B, T) | 예측된 duration () (from 3rd returned tuple)\n",
    "    log_d: (B, 1, T) | Alignment로부터 추출한 duration (from 3rd returned tuple)\n",
    "    text_len: (B) | text의 길이 (from dataset)\n",
    "    =====outputs=====\n",
    "    Loss: (1) | Duration Loss\n",
    "    \"\"\"\n",
    "    Loss = torch.sum((x_log_dur - log_d)**2) / torch.sum(text_len)\n",
    "    # MSE | torch 구현을 사용하지 않은 이유는 text_len이 모두 다르기 때문이다.\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b37a0",
   "metadata": {},
   "source": [
    "## 3.2. Adam Optimizer + Noam LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47ad0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    def __init__(self, params, dim_model=192, warmup_steps=4000, lr=1e0, betas=(0.9, 0.98)):\n",
    "        self.params = params\n",
    "        self.dim_model = dim_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "\n",
    "        self.step_num = 1\n",
    "        self.current_lr = lr * self._get_lr_scale()\n",
    "\n",
    "        self._optim = torch.optim.Adam(params, lr=self.current_lr, betas=betas)\n",
    "    \n",
    "    def _get_lr_scale(self):\n",
    "        return np.power(self.dim_model, -0.5) * np.min([np.power(self.step_num, -0.5),\n",
    "                                                        self.step_num * np.power(self.warmup_steps, -1.5)])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        self.step_num += 1\n",
    "        self.current_lr = self.lr * self._get_lr_scale() # NoamLR\n",
    "        for param_group in self._optim.param_groups:\n",
    "            param_group['lr'] = self.current_lr\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.cur_lr\n",
    "    \n",
    "    def step(self):\n",
    "        self._optim.step()\n",
    "        self._update_learning_rate()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self._optim.zero_grad()\n",
    "    \n",
    "    def load_state_dict(self, d):\n",
    "        self._optim.load_state_dict(d)\n",
    "        \n",
    "    def state_dict(self):\n",
    "        return self._optim.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445b575",
   "metadata": {},
   "source": [
    "## 3.3. Main Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4402221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name, check_step, hifi_model_name=None, hifi_check_step=None):\n",
    "    \"\"\"\n",
    "    (NEW) check_step = None\n",
    "    (LOAD) check_step: 불러오고자 하는 모델의 step\n",
    "    \n",
    "    만약 Glow-TTS로부터 생성된 Mel-spectrogram을 pretrained된 HiFi-GAN으로 학습시키길 원한다면\n",
    "    HiFi-GAN 모델명과 check_step을 적으라.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define Models & Optimizer\n",
    "    model = GlowTTS().to(device)\n",
    "    \n",
    "    optim = Adam(model.parameters())\n",
    "\n",
    "    # Load Models & Optimizer\n",
    "    epoch = 1\n",
    "    step = 1\n",
    "    if check_step is not None:\n",
    "        os.makedirs('ckpt/' + model_name, exist_ok=True)\n",
    "        check_point = './ckpt/' + model_name + \"/ckpt-\" + str(check_step) + \".pt\"\n",
    "        ckpt = torch.load(check_point)\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "        optim.load_state_dict(ckpt['optim'])\n",
    "        epoch = ckpt['epoch']\n",
    "        step = ckpt['step']\n",
    "        print(f'Load {model_name} | Step {check_step}')\n",
    "    \n",
    "    # cuDNN의 벤치마크 모드를 활성화. 합성곱과 풀링 연산 등을 최적화.\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Training Mode Activation\n",
    "    model.train()\n",
    "    \n",
    "    # tensorboard\n",
    "    os.makedirs('ckpt/' + model_name + '/logs', exist_ok=True)\n",
    "    sw = SummaryWriter('./ckpt/' + model_name + '/logs')\n",
    "    \n",
    "    # Main Training\n",
    "    start = time.time() # start time 초기화\n",
    "    while True:\n",
    "        print(f\"|| Epoch: {epoch} ||\")\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "            text = batch[0].to(device)\n",
    "            text_len = torch.tensor(batch[1], dtype=torch.int32)\n",
    "            mel = batch[2].to(device).transpose(1, 2)\n",
    "            mel_len = torch.tensor(batch[3], dtype=torch.int32)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            (z, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d) = \\\n",
    "                model(text, text_len, mel, mel_len)\n",
    "            \n",
    "            mle_loss = MLE_Loss(z, z_mean, z_log_std, log_det, z_mask)\n",
    "            duration_loss = Duration_Loss(x_log_dur, log_d, text_len)\n",
    "            loss = mle_loss + duration_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optim.step()\n",
    "            step += 1\n",
    "            \n",
    "            # Logging\n",
    "            if step % logging_step == 0:\n",
    "                print(f'| step: {step} | loss: {loss:2.3f} | mle_loss: {mle_loss:2.3f} | dur_loss: {duration_loss:2.3f} | {time.time()-start:.3f} sec / {logging_step} steps |')\n",
    "                # Tensorboard에 기록\n",
    "                sw.add_scalar(\"training/loss\", loss, step)\n",
    "                sw.add_scalar(\"training/mle_loss\", mle_loss, step)\n",
    "                sw.add_scalar(\"training/duration_loss\", duration_loss, step)\n",
    "                start = time.time() # start time 초기화\n",
    "            \n",
    "            # Validation\n",
    "            if step % validation_step == 0:\n",
    "                model.eval()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                loss_sum = 0\n",
    "                mle_loss_sum = 0\n",
    "                duration_loss_sum = 0\n",
    "                with torch.no_grad():\n",
    "                    for i, batch in enumerate(val_dataloader):\n",
    "                        text = batch[0].to(device)\n",
    "                        text_len = torch.tensor(batch[1], dtype=torch.int32)\n",
    "                        mel = batch[2].to(device).transpose(1, 2)\n",
    "                        mel_len = torch.tensor(batch[3], dtype=torch.int32)\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        (z, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d) = \\\n",
    "                            model(text, text_len, mel, mel_len)\n",
    "\n",
    "                        mle_loss = MLE_Loss(z, z_mean, z_log_std, log_det, z_mask)\n",
    "                        duration_loss = Duration_Loss(x_log_dur, log_d, text_len)\n",
    "                        loss = mle_loss + duration_loss\n",
    "                        \n",
    "                        loss_sum += loss\n",
    "                        mle_loss_sum += mle_loss\n",
    "                        duration_loss_sum += duration_loss\n",
    "                        \n",
    "                        # inference\n",
    "                        (y, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d) = \\\n",
    "                            model(text, text_len, inference=True)\n",
    "                        \n",
    "                        if i <= 4: # 그림은 5개 저장\n",
    "                            sw.add_figure(f'validation/gen_mel_{i})',\n",
    "                                          plot_mel(y[0].detach().cpu().numpy(), step, loss),\n",
    "                                          step)\n",
    "                            input_seq = sequence_to_text(text[0].cpu().numpy())\n",
    "                            input_seq = input_seq[:text_len[0]]\n",
    "                            sw.add_figure(f'validation/gen_align_{i})',\n",
    "                                          plot_alignment(attention_alignment[0].detach().cpu().numpy(), input_seq, step, loss),\n",
    "                                          step)\n",
    "                        \n",
    "                    loss_avg = loss_sum / (i+1)\n",
    "                    mle_loss_avg = mle_loss_sum / (i+1)\n",
    "                    duration_loss_avg = duration_loss_sum / (i+1)\n",
    "                    \n",
    "                    sw.add_scalar(\"validation/loss\", loss_avg, step)\n",
    "                    sw.add_scalar(\"validation/mle_loss\", mle_loss_avg, step)\n",
    "                    sw.add_scalar(\"validation/duration_loss\", duration_loss_avg, step)\n",
    "                    print(f\"Validation loss: {loss_avg}\")\n",
    "                \n",
    "                model.train()\n",
    "                start = time.time() # start time 초기화\n",
    "                        \n",
    "            # Checkpoint\n",
    "            if step % checkpoint_step == 0:\n",
    "                save_dir = './ckpt/' + model_name\n",
    "                torch.save({\n",
    "                    'model': model.state_dict(),\n",
    "                    'optim': optim.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'step': step\n",
    "                }, os.path.join(save_dir, 'ckpt-{}.pt'.format(step)))\n",
    "                \n",
    "                start = time.time() # start time 초기화\n",
    "                \n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6693163",
   "metadata": {},
   "source": [
    "## 3.4. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cee75270",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUT #####\n",
    "model_name = \"model_03\"\n",
    "check_step = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfc87408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| Epoch: 1 ||\n",
      "| step: 10 | loss: 33.380 | mle_loss: 32.023 | dur_loss: 1.357 | 17.570 sec / 10 steps |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23272\\792338283.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23272\\868959771.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model_name, check_step, hifi_model_name, hifi_check_step)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmle_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mduration_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[0mgrad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if __name__ == \"__main__\":\n",
    "    train(model_name, check_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9720bf8",
   "metadata": {},
   "source": [
    "###### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20236618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoamLR Test\n",
    "import matplotlib.pyplot as plt\n",
    "L = []\n",
    "for i in range(1, 10001):\n",
    "    lr = np.power(192, -0.5) * np.min([np.power(i, -0.5), i * np.power(4000, -1.5)])\n",
    "    L.append(lr)\n",
    "    \n",
    "plt.plot(np.arange(1, 10001), L)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
