{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7b3991",
   "metadata": {},
   "source": [
    "# Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a3823",
   "metadata": {},
   "source": [
    "- **Glow-TTS**: **dynamic programming**을 이용하여 **Text**로부터 **Mel-Spectrogram**을 생성하는 **flow 기반** 딥러닝 TTS 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3dc1cd",
   "metadata": {},
   "source": [
    "# 0. Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf1496e",
   "metadata": {},
   "source": [
    "- **Glow-TTS**는 다음과 같은 특징을 지닌 모델이다.\n",
    "    1. **Tacotron2**보다 **합성 속도**가 **15.7배 빠르다.**\n",
    "    2. 모델을 **다중 화자(multi-speaker)로 확장**하기 쉽다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e43a1",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9638a098",
   "metadata": {},
   "source": [
    "- **Glow-TTS**는 Text를 Mel-spectrogram으로 변환하는 모델이다.\n",
    "<img src=\"01.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ed8b0a",
   "metadata": {},
   "source": [
    "- **Autoregressive Model**는 초반에 딥러닝 기반 TTS 모델 중 가장 성능이 좋았다.\n",
    "    - 예) **Tacotron2**, **Transformer TTS**\n",
    "    - **단점**\n",
    "        1. **Undesirable Delay**: text 길이에 따라 추론 시간이 선형적으로 증가하다보니 real-time service에 어려움이 있음.\n",
    "        2. **A lack of robustness**: Input에 **Repeated words**가 있으면 심각한 **Attention errors**가 발생하는 등의 문제 발생\n",
    "        - 이를 극복하기 위해 **Parallel TTS Model**이 등장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3bcfcb",
   "metadata": {},
   "source": [
    "- **Parallel TTS Model**: 시간 순서에 관계없이 병렬로 음성을 생성하는 모델\n",
    "    - 예) **FastSpeech**, AlignTTS, Flow-TTS\n",
    "    - **장점**\n",
    "        1. Autoregressive model보다 **상당히 빠른 Mel-spectrogram 합성 속도**\n",
    "        2. monotonic alignment을 강제함으로써 다음과 같은 **failure cases**를 줄였다.  \n",
    "            1) Mispronouncing(잘못된 발음), 2) skipping(생략), 3) Repeating words(반복된 단어)\n",
    "    - **단점**: **External aligners의 성능에 크게 의존**한다는 문제가 있었다.\n",
    "        - **External aligners(외부 정렬기)**라 불리는 **pre-trained autoregressive models**로부터 text와 speech사이의 **well-aligned attention maps**를 추출해야만 했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f2d6b",
   "metadata": {},
   "source": [
    "- **Glow-TTS**: **External aligner를 제거**한 **flow-based generative model(parallel TTS model의 일종)**\n",
    "    1. **flows**와 **dynamic programming**을 이용해서 내부적으로 적절한 **monotonic alignment**을 탐색하고 강제한다.\n",
    "        - 이는 **긴 발화를 생성하는 견고한 TTS 모델**을 만드는 데에 기여한다.\n",
    "        - flows와 dynamic programming에 대한 설명은 이후에 언급한다.\n",
    "    2. **flows**를 이용하여 **빠르고, 다양하고, 음성 합성 제어가 가능하게 한다.**\n",
    "        - **Tacotron2보다 15.7배 빠른 합성 속도**를 갖는다.\n",
    "        - **latent representation**을 변경함으로써 **다양한 억양 패턴을 합성**할 수 있고, **음 높이를 조정**할 수 있다.\n",
    "        - 여기서 작은 수정만으로 **다중 화자** 지원이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3faf9",
   "metadata": {},
   "source": [
    "# 2. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a687861",
   "metadata": {},
   "source": [
    "- 아래 내용은 논문에서 직접적으로 언급되지는 않지만, 논문의 내용을 이해하기 위해 필요한 배경 지식을 설명한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0556350d",
   "metadata": {},
   "source": [
    "- **Flow-based generative model**: 일대일 변환 함수를 이용하여 입력 데이터와 잠재 변수 간의 확률 분포를 예측하는 모델\n",
    "<img src=\"02.png\" width=70% height=70%>\n",
    "(출처: https://lilianweng.github.io/posts/2018-10-13-flow-models/)\n",
    "\n",
    "    - **GAN(Generative Adversarial Networks)**: 생성자(generator)와 판별자(discriminator)의 경쟁적인 학습을 이용하는 딥러닝 생성 모델\n",
    "    - **VAE(Variational Auto Encoder)**: 입력 데이터를 잠재 공간으로 압축한 다음, 잠재 공간에서 다시 원래 데이터로 복원하는 과정에서 간접적으로 데이터의 분포를 학습하여 새로운 데이터를 생성하는 딥러닝 생성 모델. Evidence lower bound (ELBO)를 손실함수로 사용한다.\n",
    "    - **Flow-based generative model**: VAE와는 달리 명시적으로 데이터 분포를 학습하며, **가역 함수**를 이용하므로 데이터 손실이 적다.\n",
    "    \n",
    "    <img src=\"03.png\" width=70% height=70%>\n",
    "    (출처: https://lilianweng.github.io/posts/2018-10-13-flow-models/)\n",
    "    \n",
    "        - **flow 함수**는 가역 함수들의 합성 함수로 구성된다.\n",
    "        - **잠재 공간**의 단순한 확률 분포에서 flow 함수를 거치면서 **실제 데이터의 복잡한 확률 분포**를 얻을 수 있도록 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab6d7e",
   "metadata": {},
   "source": [
    "- **Dynamic Programming(동적 계획법)**: 특정 문제를 풀기 위해 다른 문제의 답을 기억하면서 이용하는 방법\n",
    "<img src=\"04.png\" width=70% height=70%>\n",
    "\n",
    "    - 위 그림과 같이 정의된 피보나치 수열을 구하려고 한다. 즉, F(4)를 구하려고 한다.\n",
    "        - **재귀적 방법(Recursive Algorithm)**은 F(4)를 구하기 위해 **4번 연산**해야 한다.\n",
    "        - 그러나 **F(2)를 기억하는 동적 계획법(Dynamic Programming)**을 이용하면 F(2)를 다시 계산하지 않아도 되므로 **3번만 연산**하면 된다.\n",
    "    - 더 많은 재귀 호출이 일어날 수록 dynamic programming과 기하급수적으로 연산량에 차이를 보인다.\n",
    "    \n",
    "- Dynamic Programming의 두 가지 방식\n",
    "    - **Top-down**: F(4) -> F(3) -> F(2) 순서로 계산\n",
    "    - **Bottom-up**: F(2) -> F(3) -> F(4) 순서로 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87c9ee",
   "metadata": {},
   "source": [
    "# 3. Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f640a8",
   "metadata": {},
   "source": [
    "## 3.1. Training and Inference Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ce1e3",
   "metadata": {},
   "source": [
    "### 3.1.1. Training Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf0990b",
   "metadata": {},
   "source": [
    "- **Glow-TTS**의 **훈련 절차**는 다음과 같다.\n",
    "<img src=\"05.png\" width=50% height=50%>\n",
    "\n",
    "    - $T_{text}$: the length of the text input, $T_{mel}$: the length of the input mel-spectrogram\n",
    "    - $c = c_{1:T_{text}}$: text, $z = z_{1:T_{mel}}$: latent representation, $x = x_{1:T_{mel}}$: mel-spectrogram\n",
    "    - $P_{Z}(z|c)$: $c$에 대한 $z$의 확률 분포, $P_{X}(x|c)$: $c$에 대한 $x$의 확률 분포\n",
    "    - $A$: alignment function, $f_{enc}(c)=(\\mu, \\sigma)$, $f_{dec}(z)=x$라 하자. 이때 $f_{dec}$는 **가역 함수**, $A$는 **단조(monotonic) 전사(surjective)함수**라 가정한다.\n",
    "    - 사전 확률 분포 $P_{Z}$는 **isotropic multivariate Gaussian distribution(공분산 행렬이 항등행렬인 다변량 정규분포)**라 가정하자.\n",
    "        - $z=(z_{1}, \\dots, z_{T_{text}})$는 공분산 행렬이 항등행렬이고 각 변수가 정규분포이므로 서로 **mutually independent**이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68af52e",
   "metadata": {},
   "source": [
    "- 이제 Training을 위한 식을 전개해보자. (논문에는 중간 과정이 생략되어 있다.)\n",
    "    - 변수 변환(Change of variable formula)에 의해 다음 식이 성립한다.\n",
    "    $$ P_{X}(x|c) = P_{Z}(f^{-1}(x)|c)\\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) $$\n",
    "    - $f^{-1}(x)=z$이므로 양변에 로그를 취하면 다음과 같다.\n",
    "    $$ \\log P_{X}(x|c) = \\log P_{Z}(z|c) + \\log \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| \\cdots(1)$$\n",
    "    - $z_{j} \\sim N(z_{j};\\mu_{i}, \\sigma_{i})$이면 $A(j) = i$이라 하자. 그러면 다음 식이 성립한다.\n",
    "    $$ P_{z_{j}}(z_{j}|c;\\theta, A) = N(z_{j};\\mu_{A(j)}, \\sigma_{A(j)}) \\cdots(2)$$\n",
    "    - $z$는 각 변수에 대해 독립이므로 다음 식이 성립한다.\n",
    "    $$ P_{z}(z|c;\\theta, A) = P_{z_{1}}(z_{1}|c;\\theta, A)P_{z_{2}}(z_{2}|c;\\theta, A) \\cdots P_{z_{T_{mel}}}(z_{T_{mel}}|c;\\theta, A) = \\prod_{j=1}^{T_{mel}} P_{z_{j}}(z_{j}|c;\\theta, A) \\cdots (3)$$\n",
    "    - 식 (3)에 식 (2)을 적용하고 양변에 로그를 취하면 다음과 같다.\n",
    "    $$ \\log P_{Z}(z|c;\\theta, A) = \\sum_{j=1}^{T_{mel}} \\log N(z_{j};\\mu_{A(j)}, \\sigma_{A(j)}) \\cdots (4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12eff57",
   "metadata": {},
   "source": [
    "- 우리는 $P_{X}(x|c)$가 최대가 되길 원한다. 즉, log-likelihood $L(\\theta, A) = \\log P_{X}(x|c;A, \\theta)$가 최대가 되는 $\\theta$와 $A$를 찾고 싶다.\n",
    "    - 그러나 $\\max_{\\theta, A} L(\\theta, A) = \\max_{\\theta, A} \\log P_{X}(x|c;A, \\theta)$의 global solution을 찾는 것은 계산적으로 매우 어렵다.\n",
    "    - 그래서 이를 구하는 과정을 두 단계로 나눈다.\n",
    "        1. 현재 $\\theta$에 대한 가장 적절한 $A^{*}$를 찾는다.\n",
    "            - 식 (1)와 식(4)에 의해 식은 다음과 같이 정리된다.\n",
    "            $$ A^{*}=\\arg\\max_{A} \\log P_{X}(x|c;A, \\theta) = \\arg\\max_{A} \\sum_{j=1}^{T_{mel}} \\log N(z_{j}; \\mu_{A(j)}, \\sigma_{A(j)}) \\cdots (5)$$\n",
    "        2. $\\log P_{X}(x|c;\\theta, A^{*})$를 최대화하도록 $\\theta$를 업데이트한다.\n",
    "    - 위 1번 과정은 이후에 소개할 **Monotonic Alignment Search(MAS) Algorithm**으로 해결할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d7d9f",
   "metadata": {},
   "source": [
    "- **Duration Predictor**: 더 적절한 alignment $A^{*}$를 추정하기 위해서 **각 text에 대응되는 alignment의 길이(duration)를 예측**하는 함수\n",
    "    - $f_{dur}$: Duration predictor, $sg[\\cdot]$: stop gradient operator(gradient를 backward 전파하지 않도록 함.)이라 하자.\n",
    "    - 이때 Duration predictor의 Loss는 다음과 같이 설정한다.\n",
    "    $$d_{i} = \\sum_{j=1}^{T_{mel}} 1_{A*(j)=i}, \\, i = 1, \\cdots, T_{text}$$\n",
    "    $$L_{dur} = MSE(f_{dur}(sg[f_{enc}(c)]), d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ccc848",
   "metadata": {},
   "source": [
    "### 3.1.2. Inference Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4fe1e",
   "metadata": {},
   "source": [
    "- **Glow-TTS**의 **추론 절차**는 다음과 같다.\n",
    "<img src=\"06.png\" width=50% height=50%>\n",
    "\n",
    "    - Alignment는 **Duration Predictor**의 결과로부터 Ceil(올림)하여 결정한다.\n",
    "    - latent representation은 Encoder로부터 추론된 prior distribution으로부터 sampling한다.\n",
    "    - Mel-spectrogram은 flow-based Decoder로부터 생성된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c179e9f",
   "metadata": {},
   "source": [
    "## 3.2. Monotonic Alignment Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999be44",
   "metadata": {},
   "source": [
    "- 다음은 Monotonic Alignment의 예시이다.\n",
    "<img src=\"07.png\" width=50% height=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ca2f0",
   "metadata": {},
   "source": [
    "- $Q_{i, j}$를 $i$번째 prior distribution와 $j$번째 latent variable까지 주어졌을 때의 **최대 로그 우도**라 하자.\n",
    "<img src=\"08.png\" width=50% height=50%>\n",
    "    \n",
    "- **(Step 1)**\n",
    "    - 그러면 $z_{j}$의 이전 latent variable은 $z_{j-1}$이고, $A$의 **monotonicity**와 **surjection**에 의해 $z_{j-1}$은 $(\\mu_{i-1}, \\sigma_{i-1})$ 또는 $(\\mu_{i}, \\sigma_{i})$ 에 의해 정렬된다.\n",
    "    - 따라서 식(5)에 의해 $Q_{i, j}$가 다음과 같이 재귀적으로 결정된다.\n",
    "    $$Q_{i, j} = \\max_{A} \\sum_{k=1}^{j} \\log N(z_{k}; \\mu_{A(k)}, \\sigma_{A(k)})=\\max(Q_{i-1, j-1}, Q_{i, j-1})+\\log N(z_{j}; \\mu_{i}, \\sigma_{i})$$\n",
    "    - 이러한 방식으로 $Q_{T_{text}, T_{mel}}$까지 계산할 수 있다.  \n",
    "    \n",
    "- **(Step 2)**\n",
    "    - 그 다음 $A^{*}$를 결정하기 위해서 **Backtracking**을 진행한다.\n",
    "    <img src=\"09.png\" width=60% height=60%>\n",
    "    \n",
    "    - $A^{*}(T_{mel})=T_{text}$로 둔 다음에 backtracking하여 Alignment를 추정해간다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e9ab2",
   "metadata": {},
   "source": [
    "- 그런데 이를 재귀적인 방법으로 시도하면 상당히 많은 overhead가 발생한다!\n",
    "- 위 알고리즘에 **Dynamic Programming**을 적용한 것이 **Monotonic Alignmnet Search Algorithm**이다.\n",
    "<img src=\"10.png\" width=70% height=70%>\n",
    "\n",
    "- 알고리즘을 그림으로 정리해보자.\n",
    "<img src=\"11.png\" width=50% height=50%>\n",
    "<img src=\"12.png\" width=50% height=50%>\n",
    "<img src=\"13.png\" width=50% height=50%>\n",
    "\n",
    "    - 이후에 Backtracking을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0bfefa",
   "metadata": {},
   "source": [
    "## 3.3. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4611a6",
   "metadata": {},
   "source": [
    "**[참고 자료]**\n",
    "1. Glow-TTS 논문: https://arxiv.org/abs/2005.11129\n",
    "2. Flow-based generative model github blog: https://lilianweng.github.io/posts/2018-10-13-flow-models/\n",
    "3. Flow-based generative model tistory blog: https://sanghyu.tistory.com/18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fb60e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
